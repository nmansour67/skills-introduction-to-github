{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxzCMqj8eLK225SUw+jTIc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nmansour67/skills-introduction-to-github/blob/main/Ethical_Firewell_Digital_Eugenicist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0xLedqOKCtu",
        "outputId": "28ac67e1-99b8-48f7-cc38-1e665e2ad7ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "âš ï¸  THE ETHICAL FIREWALL: Stopping the Digital Eugenicist\n",
            "================================================================================\n",
            "\n",
            "ğŸ“š Learning Goal: Understand how AI can automate inequality in healthcare\n",
            "   and learn to detect and prevent algorithmic bias\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "THE ETHICAL FIREWALL: Stopping the \"Digital Eugenicist\"\n",
        "=========================================================\n",
        "Teaching Goal: Show physicians how AI can automate inequality and how to stop it\n",
        "Author: Expert AI Ethics Teacher\n",
        "Audience: Healthcare professionals learning AI ethics\n",
        "WARNING: This demonstration contains realistic bias scenarios\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import shap\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)  # For reproducibility\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"âš ï¸  THE ETHICAL FIREWALL: Stopping the Digital Eugenicist\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nğŸ“š Learning Goal: Understand how AI can automate inequality in healthcare\")\n",
        "print(\"   and learn to detect and prevent algorithmic bias\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 1: Create a Biased Dataset That Mirrors Real-World Inequality\n",
        "# ============================================================================\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 1: Creating Biased Healthcare Dataset\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nâš ï¸  THE CRUEL IRONY: Historical data encodes structural inequality\")\n",
        "print(\"   Marginalized patients â†’ Less access â†’ Lower spending\")\n",
        "print(\"   But AI interprets: Lower spending â†’ Lower need â†’ Deny care\")\n",
        "print(\"   Result: The sickest patients are denied care because they're poor\\n\")\n",
        "\n",
        "# Create 20 patients with deliberately biased patterns\n",
        "data = {\n",
        "    'Patient_ID': [f'P{str(i).zfill(3)}' for i in range(1, 21)],\n",
        "\n",
        "    # Demographics and social determinants\n",
        "    'Race': ['White', 'White', 'White', 'White', 'White', 'White',\n",
        "             'Black', 'Black', 'Black', 'Black',\n",
        "             'Hispanic', 'Hispanic', 'Hispanic', 'Hispanic',\n",
        "             'Asian', 'Asian', 'White', 'Black', 'Hispanic', 'White'],\n",
        "\n",
        "    'Zip_Code': [10021, 90210, 10021, 90210, 10021, 10021,  # Wealthy areas\n",
        "                 10456, 10456, 90001, 90001,  # Underserved areas\n",
        "                 90001, 10456, 90001, 10456,  # Underserved areas\n",
        "                 10021, 90210, 10021, 10456, 90001, 90210],\n",
        "\n",
        "    'Insurance_Type': ['Private', 'Private', 'Private', 'Private', 'Private', 'Private',\n",
        "                       'Medicaid', 'Uninsured', 'Medicaid', 'Medicaid',\n",
        "                       'Uninsured', 'Medicaid', 'Medicaid', 'Uninsured',\n",
        "                       'Private', 'Private', 'Private', 'Medicaid', 'Uninsured', 'Private'],\n",
        "\n",
        "    # THE BIAS: Prior spending reflects ACCESS, not NEED\n",
        "    # Wealthy patients have HIGH spending even if healthy\n",
        "    # Poor patients have LOW spending even if sick\n",
        "    'Prior_Healthcare_Spending': [8500, 9200, 7800, 8900, 9500, 8200,  # Wealthy â†’ High spending\n",
        "                                  1200, 800, 1500, 1100,   # Poor â†’ Low spending\n",
        "                                  900, 1300, 1000, 700,    # Poor â†’ Low spending\n",
        "                                  8700, 9100, 8400, 1400, 950, 8600],\n",
        "\n",
        "    # THE TRUTH: Actual disease severity (clinical need)\n",
        "    # Note: Marginalized patients often have HIGHER severity due to delayed care\n",
        "    'Actual_Disease_Severity': [3, 2, 4, 3, 2, 3,   # Wealthy: Mild-moderate (good preventive care)\n",
        "                                8, 9, 8, 9,          # Poor: Severe (delayed care, chronic stress)\n",
        "                                9, 8, 9, 8,          # Poor: Severe\n",
        "                                4, 3, 2, 9, 8, 3],\n",
        "\n",
        "    # Hospital visits (also biased by access - ER visits vs preventive care)\n",
        "    'Hospital_Visits': [5, 4, 6, 5, 4, 5,    # Wealthy: Regular preventive visits\n",
        "                       2, 1, 2, 2,           # Poor: Avoid hospitals (cost, distrust)\n",
        "                       1, 2, 1, 1,           # Poor: Avoid hospitals\n",
        "                       6, 5, 4, 2, 1, 5],\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# The BIASED historical enrollment decision (based on spending, not severity)\n",
        "# This is what the AI will learn: \"Rich people get care, poor people don't\"\n",
        "df['Historical_Enrollment'] = (df['Prior_Healthcare_Spending'] > 5000).astype(int)\n",
        "\n",
        "print(\"ğŸ“Š Patient Dataset (showing the structural bias):\\n\")\n",
        "print(df[['Patient_ID', 'Race', 'Insurance_Type', 'Prior_Healthcare_Spending',\n",
        "          'Actual_Disease_Severity', 'Historical_Enrollment']].to_string(index=False))\n",
        "\n",
        "print(\"\\nâš ï¸  NOTICE THE INJUSTICE:\")\n",
        "print(\"   â€¢ Patients P007-P014: High disease severity (8-9) but LOW historical enrollment\")\n",
        "print(\"   â€¢ Why? Low spending due to barriers (Medicaid, uninsured, underserved areas)\")\n",
        "print(\"   â€¢ Patients P001-P006: Mild severity (2-4) but HIGH historical enrollment\")\n",
        "print(\"   â€¢ Why? High spending because they can afford care\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhKqE3U0MgWV",
        "outputId": "e77d3b9a-2f39-43a0-aa33-2203a0397bd5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 1: Creating Biased Healthcare Dataset\n",
            "================================================================================\n",
            "\n",
            "âš ï¸  THE CRUEL IRONY: Historical data encodes structural inequality\n",
            "   Marginalized patients â†’ Less access â†’ Lower spending\n",
            "   But AI interprets: Lower spending â†’ Lower need â†’ Deny care\n",
            "   Result: The sickest patients are denied care because they're poor\n",
            "\n",
            "ğŸ“Š Patient Dataset (showing the structural bias):\n",
            "\n",
            "Patient_ID     Race Insurance_Type  Prior_Healthcare_Spending  Actual_Disease_Severity  Historical_Enrollment\n",
            "      P001    White        Private                       8500                        3                      1\n",
            "      P002    White        Private                       9200                        2                      1\n",
            "      P003    White        Private                       7800                        4                      1\n",
            "      P004    White        Private                       8900                        3                      1\n",
            "      P005    White        Private                       9500                        2                      1\n",
            "      P006    White        Private                       8200                        3                      1\n",
            "      P007    Black       Medicaid                       1200                        8                      0\n",
            "      P008    Black      Uninsured                        800                        9                      0\n",
            "      P009    Black       Medicaid                       1500                        8                      0\n",
            "      P010    Black       Medicaid                       1100                        9                      0\n",
            "      P011 Hispanic      Uninsured                        900                        9                      0\n",
            "      P012 Hispanic       Medicaid                       1300                        8                      0\n",
            "      P013 Hispanic       Medicaid                       1000                        9                      0\n",
            "      P014 Hispanic      Uninsured                        700                        8                      0\n",
            "      P015    Asian        Private                       8700                        4                      1\n",
            "      P016    Asian        Private                       9100                        3                      1\n",
            "      P017    White        Private                       8400                        2                      1\n",
            "      P018    Black       Medicaid                       1400                        9                      0\n",
            "      P019 Hispanic      Uninsured                        950                        8                      0\n",
            "      P020    White        Private                       8600                        3                      1\n",
            "\n",
            "âš ï¸  NOTICE THE INJUSTICE:\n",
            "   â€¢ Patients P007-P014: High disease severity (8-9) but LOW historical enrollment\n",
            "   â€¢ Why? Low spending due to barriers (Medicaid, uninsured, underserved areas)\n",
            "   â€¢ Patients P001-P006: Mild severity (2-4) but HIGH historical enrollment\n",
            "   â€¢ Why? High spending because they can afford care\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 2: Train a \"Biased AI\" That Learns Historical Inequality\n",
        "# ============================================================================\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 2: Training the 'Digital Eugenicist' Model\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nğŸ¤– We train an AI on historical data to predict care enrollment\")\n",
        "print(\"   The AI will learn: 'Give care to people who already got care'\")\n",
        "print(\"   This perpetuates inequality at algorithmic scale\\n\")\n",
        "\n",
        "# Prepare features for the BIASED model (includes all the problematic features)\n",
        "# Encode categorical variables\n",
        "le_race = LabelEncoder()\n",
        "le_insurance = LabelEncoder()\n",
        "\n",
        "df['Race_Encoded'] = le_race.fit_transform(df['Race'])\n",
        "df['Insurance_Encoded'] = le_insurance.fit_transform(df['Insurance_Type'])\n",
        "\n",
        "# Features for BIASED model - includes proxy discrimination variables\n",
        "biased_features = ['Race_Encoded', 'Zip_Code', 'Insurance_Encoded',\n",
        "                   'Prior_Healthcare_Spending', 'Actual_Disease_Severity', 'Hospital_Visits']\n",
        "\n",
        "X_biased = df[biased_features]\n",
        "y = df['Historical_Enrollment']\n",
        "\n",
        "# Train the biased model\n",
        "biased_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5)\n",
        "biased_model.fit(X_biased, y)\n",
        "\n",
        "# Get predictions\n",
        "df['Biased_Model_Prediction'] = biased_model.predict(X_biased)\n",
        "df['Biased_Model_Probability'] = biased_model.predict_proba(X_biased)[:, 1]\n",
        "\n",
        "print(\"âœ… Biased Model Trained\")\n",
        "print(\"   Features used: Race, Zip Code, Insurance, Prior Spending, Disease Severity, Visits\")\n",
        "print(\"\\nğŸ“ˆ Model Predictions vs Reality:\\n\")\n",
        "\n",
        "# Show the tragic outcomes\n",
        "comparison = df[['Patient_ID', 'Race', 'Insurance_Type', 'Actual_Disease_Severity',\n",
        "                 'Biased_Model_Prediction']].copy()\n",
        "comparison['Outcome'] = comparison.apply(\n",
        "    lambda row: 'âŒ DENIED (Sick but poor!)' if row['Actual_Disease_Severity'] >= 8 and row['Biased_Model_Prediction'] == 0\n",
        "                else 'âœ… APPROVED (Healthy & rich)' if row['Actual_Disease_Severity'] <= 4 and row['Biased_Model_Prediction'] == 1\n",
        "                else 'âœ… APPROVED' if row['Biased_Model_Prediction'] == 1\n",
        "                else 'âŒ DENIED',\n",
        "    axis=1\n",
        ")\n",
        "print(comparison.to_string(index=False))\n",
        "\n",
        "# Count the injustice\n",
        "sick_denied = ((df['Actual_Disease_Severity'] >= 8) & (df['Biased_Model_Prediction'] == 0)).sum()\n",
        "healthy_approved = ((df['Actual_Disease_Severity'] <= 4) & (df['Biased_Model_Prediction'] == 1)).sum()\n",
        "\n",
        "print(f\"\\nğŸ’” THE INJUSTICE IN NUMBERS:\")\n",
        "print(f\"   â€¢ {sick_denied} severely sick patients (severity â‰¥8) DENIED care\")\n",
        "print(f\"   â€¢ {healthy_approved} mildly sick patients (severity â‰¤4) APPROVED for care\")\n",
        "print(f\"   â€¢ The AI is rationing healthcare BACKWARDS\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILdx7jJBMkf4",
        "outputId": "1417ec2a-bfc4-4d2d-8bf8-1b2f23890b30"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 2: Training the 'Digital Eugenicist' Model\n",
            "================================================================================\n",
            "\n",
            "ğŸ¤– We train an AI on historical data to predict care enrollment\n",
            "   The AI will learn: 'Give care to people who already got care'\n",
            "   This perpetuates inequality at algorithmic scale\n",
            "\n",
            "âœ… Biased Model Trained\n",
            "   Features used: Race, Zip Code, Insurance, Prior Spending, Disease Severity, Visits\n",
            "\n",
            "ğŸ“ˆ Model Predictions vs Reality:\n",
            "\n",
            "Patient_ID     Race Insurance_Type  Actual_Disease_Severity  Biased_Model_Prediction                     Outcome\n",
            "      P001    White        Private                        3                        1 âœ… APPROVED (Healthy & rich)\n",
            "      P002    White        Private                        2                        1 âœ… APPROVED (Healthy & rich)\n",
            "      P003    White        Private                        4                        1 âœ… APPROVED (Healthy & rich)\n",
            "      P004    White        Private                        3                        1 âœ… APPROVED (Healthy & rich)\n",
            "      P005    White        Private                        2                        1 âœ… APPROVED (Healthy & rich)\n",
            "      P006    White        Private                        3                        1 âœ… APPROVED (Healthy & rich)\n",
            "      P007    Black       Medicaid                        8                        0   âŒ DENIED (Sick but poor!)\n",
            "      P008    Black      Uninsured                        9                        0   âŒ DENIED (Sick but poor!)\n",
            "      P009    Black       Medicaid                        8                        0   âŒ DENIED (Sick but poor!)\n",
            "      P010    Black       Medicaid                        9                        0   âŒ DENIED (Sick but poor!)\n",
            "      P011 Hispanic      Uninsured                        9                        0   âŒ DENIED (Sick but poor!)\n",
            "      P012 Hispanic       Medicaid                        8                        0   âŒ DENIED (Sick but poor!)\n",
            "      P013 Hispanic       Medicaid                        9                        0   âŒ DENIED (Sick but poor!)\n",
            "      P014 Hispanic      Uninsured                        8                        0   âŒ DENIED (Sick but poor!)\n",
            "      P015    Asian        Private                        4                        1 âœ… APPROVED (Healthy & rich)\n",
            "      P016    Asian        Private                        3                        1 âœ… APPROVED (Healthy & rich)\n",
            "      P017    White        Private                        2                        1 âœ… APPROVED (Healthy & rich)\n",
            "      P018    Black       Medicaid                        9                        0   âŒ DENIED (Sick but poor!)\n",
            "      P019 Hispanic      Uninsured                        8                        0   âŒ DENIED (Sick but poor!)\n",
            "      P020    White        Private                        3                        1 âœ… APPROVED (Healthy & rich)\n",
            "\n",
            "ğŸ’” THE INJUSTICE IN NUMBERS:\n",
            "   â€¢ 10 severely sick patients (severity â‰¥8) DENIED care\n",
            "   â€¢ 10 mildly sick patients (severity â‰¤4) APPROVED for care\n",
            "   â€¢ The AI is rationing healthcare BACKWARDS\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 3: Use SHAP to Reveal What the AI Learned\n",
        "# ============================================================================\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 3: Opening the Black Box with SHAP\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nğŸ” Let's see which features the AI actually used to make decisions\\n\")\n",
        "\n",
        "# Calculate SHAP values\n",
        "explainer = shap.TreeExplainer(biased_model)\n",
        "shap_values = explainer.shap_values(X_biased)\n",
        "\n",
        "# For binary classification, get SHAP values for the positive class (enrollment)\n",
        "if isinstance(shap_values, list):\n",
        "    shap_values_positive = shap_values[1]\n",
        "else:\n",
        "    shap_values_positive = shap_values\n",
        "\n",
        "# Calculate mean absolute SHAP values (global feature importance)\n",
        "# Handle both 2D and 3D SHAP arrays\n",
        "if shap_values_positive.ndim == 3:\n",
        "    # Shape is (n_samples, n_features, n_outputs)\n",
        "    mean_abs_shap = np.abs(shap_values_positive[:, :, 0]).mean(axis=0)\n",
        "elif shap_values_positive.ndim == 2:\n",
        "    # Shape is (n_samples, n_features)\n",
        "    mean_abs_shap = np.abs(shap_values_positive).mean(axis=0)\n",
        "else:\n",
        "    mean_abs_shap = np.abs(shap_values_positive).flatten()\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': biased_features,\n",
        "    'Mean_Absolute_SHAP': mean_abs_shap\n",
        "}).sort_values('Mean_Absolute_SHAP', ascending=False)\n",
        "\n",
        "print(\"ğŸ“Š Feature Importance (What the AI actually uses):\\n\")\n",
        "for idx, row in feature_importance.iterrows():\n",
        "    feature = row['Feature']\n",
        "    importance = row['Mean_Absolute_SHAP']\n",
        "\n",
        "    # Decode feature names for readability\n",
        "    if feature == 'Race_Encoded':\n",
        "        feature_display = 'ğŸš¨ RACE'\n",
        "    elif feature == 'Zip_Code':\n",
        "        feature_display = 'ğŸš¨ ZIP CODE'\n",
        "    elif feature == 'Insurance_Encoded':\n",
        "        feature_display = 'ğŸš¨ INSURANCE TYPE'\n",
        "    elif feature == 'Prior_Healthcare_Spending':\n",
        "        feature_display = 'ğŸš¨ PRIOR SPENDING (proxy for wealth)'\n",
        "    else:\n",
        "        feature_display = feature\n",
        "\n",
        "    print(f\"   {feature_display:45s} â†’ Importance: {importance:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqdIKADQMo5g",
        "outputId": "a3dc4f70-2187-4cd9-86f3-6525b4c40174"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 3: Opening the Black Box with SHAP\n",
            "================================================================================\n",
            "\n",
            "ğŸ” Let's see which features the AI actually used to make decisions\n",
            "\n",
            "ğŸ“Š Feature Importance (What the AI actually uses):\n",
            "\n",
            "   Hospital_Visits                               â†’ Importance: 0.1457\n",
            "   ğŸš¨ PRIOR SPENDING (proxy for wealth)           â†’ Importance: 0.1395\n",
            "   Actual_Disease_Severity                       â†’ Importance: 0.1382\n",
            "   ğŸš¨ RACE                                        â†’ Importance: 0.0643\n",
            "   ğŸš¨ ZIP CODE                                    â†’ Importance: 0.0123\n",
            "   ğŸš¨ INSURANCE TYPE                              â†’ Importance: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 4: THE \"RED FACE TEST\" - Ethical Firewall\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4: THE 'RED FACE TEST' - Ethical Firewall Audit\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nğŸ›¡ï¸  Before deploying ANY model, we run the Red Face Test\")\n",
        "print(\"   Rule: If unethical features are in top 3 â†’ BURN THE MODEL\\n\")\n",
        "\n",
        "# Get top 3 features\n",
        "top_3_features = feature_importance.head(3)['Feature'].tolist()\n",
        "\n",
        "print(\"ğŸ”¥ RED FACE TEST RESULTS:\\n\")\n",
        "print(\"   Top 3 Features Driving Model Decisions:\")\n",
        "for i, feature in enumerate(top_3_features, 1):\n",
        "    importance = feature_importance[feature_importance['Feature'] == feature]['Mean_Absolute_SHAP'].values[0]\n",
        "    print(f\"   #{i}: {feature} (Importance: {importance:.4f})\")\n",
        "\n",
        "# Check for ethical violations\n",
        "violations = []\n",
        "if 'Race_Encoded' in top_3_features:\n",
        "    violations.append('RACE')\n",
        "if 'Zip_Code' in top_3_features:\n",
        "    violations.append('ZIP CODE (redlining)')\n",
        "if 'Insurance_Encoded' in top_3_features:\n",
        "    violations.append('INSURANCE TYPE (wealth discrimination)')\n",
        "if 'Prior_Healthcare_Spending' in top_3_features:\n",
        "    violations.append('PRIOR SPENDING (proxy for wealth/access)')\n",
        "\n",
        "if violations:\n",
        "    print(\"\\n\" + \"ğŸ”¥\" * 40)\n",
        "    print(\"ğŸ”¥\" * 40)\n",
        "    print(\"ğŸš¨ RED FACE TEST: FAILED - ETHICAL VIOLATIONS DETECTED\")\n",
        "    print(\"ğŸ”¥\" * 40)\n",
        "    print(\"ğŸ”¥\" * 40)\n",
        "    print(\"\\nâ›” BURN THIS MODEL â›”\")\n",
        "    print(\"\\n   Violations found:\")\n",
        "    for violation in violations:\n",
        "        print(f\"   â€¢ {violation}\")\n",
        "    print(\"\\nğŸ’€ This model is a DIGITAL EUGENICIST\")\n",
        "    print(\"   It automates inequality, denying care to the most vulnerable\")\n",
        "    print(\"   It violates: Title VI Civil Rights Act, ACA Section 1557\")\n",
        "    print(\"   Legal risk: Discrimination lawsuits, CMS penalties\")\n",
        "    print(\"   Moral risk: Patient deaths from denied care\")\n",
        "    print(\"\\nğŸ›‘ DO NOT DEPLOY THIS MODEL\")\n",
        "    print(\"ğŸ”¥\" * 40 + \"\\n\")\n",
        "else:\n",
        "    print(\"\\nâœ… RED FACE TEST: PASSED\")\n",
        "    print(\"   No unethical proxy features detected in top 3\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOLNLxe9MrW4",
        "outputId": "86401ee6-9594-4f35-96f4-6ed1c6a04c87"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 4: THE 'RED FACE TEST' - Ethical Firewall Audit\n",
            "================================================================================\n",
            "\n",
            "ğŸ›¡ï¸  Before deploying ANY model, we run the Red Face Test\n",
            "   Rule: If unethical features are in top 3 â†’ BURN THE MODEL\n",
            "\n",
            "ğŸ”¥ RED FACE TEST RESULTS:\n",
            "\n",
            "   Top 3 Features Driving Model Decisions:\n",
            "   #1: Hospital_Visits (Importance: 0.1457)\n",
            "   #2: Prior_Healthcare_Spending (Importance: 0.1395)\n",
            "   #3: Actual_Disease_Severity (Importance: 0.1382)\n",
            "\n",
            "ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥\n",
            "ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥\n",
            "ğŸš¨ RED FACE TEST: FAILED - ETHICAL VIOLATIONS DETECTED\n",
            "ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥\n",
            "ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥\n",
            "\n",
            "â›” BURN THIS MODEL â›”\n",
            "\n",
            "   Violations found:\n",
            "   â€¢ PRIOR SPENDING (proxy for wealth/access)\n",
            "\n",
            "ğŸ’€ This model is a DIGITAL EUGENICIST\n",
            "   It automates inequality, denying care to the most vulnerable\n",
            "   It violates: Title VI Civil Rights Act, ACA Section 1557\n",
            "   Legal risk: Discrimination lawsuits, CMS penalties\n",
            "   Moral risk: Patient deaths from denied care\n",
            "\n",
            "ğŸ›‘ DO NOT DEPLOY THIS MODEL\n",
            "ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 5: Show Individual Patient Harm\n",
        "# ============================================================================\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 5: Individual Patient Examples - The Human Cost\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nğŸ’” Let's see specific patients harmed by this biased AI:\\n\")\n",
        "\n",
        "# Find severely sick patients denied care\n",
        "denied_sick = df[(df['Actual_Disease_Severity'] >= 8) & (df['Biased_Model_Prediction'] == 0)].head(3)\n",
        "\n",
        "for idx, patient in denied_sick.iterrows():\n",
        "    patient_id = patient['Patient_ID']\n",
        "    race = patient['Race']\n",
        "    insurance = patient['Insurance_Type']\n",
        "    severity = patient['Actual_Disease_Severity']\n",
        "    spending = patient['Prior_Healthcare_Spending']\n",
        "\n",
        "    print(f\"âŒ {patient_id}: {race}, {insurance}\")\n",
        "    print(f\"   Disease Severity: {severity}/10 (CRITICALLY ILL)\")\n",
        "    print(f\"   Prior Spending: ${spending:,}\")\n",
        "    print(f\"   AI Decision: DENIED care management\")\n",
        "    print(f\"   Why? Low spending interpreted as 'low need'\")\n",
        "    print(f\"   Reality: Low spending due to cost barriers, not health\")\n",
        "    print(f\"   Outcome: Sick patient denied life-saving resources\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q00a95bkMuBt",
        "outputId": "cbe335bd-e441-493e-9d3c-88476fb224c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 5: Individual Patient Examples - The Human Cost\n",
            "================================================================================\n",
            "\n",
            "ğŸ’” Let's see specific patients harmed by this biased AI:\n",
            "\n",
            "âŒ P007: Black, Medicaid\n",
            "   Disease Severity: 8/10 (CRITICALLY ILL)\n",
            "   Prior Spending: $1,200\n",
            "   AI Decision: DENIED care management\n",
            "   Why? Low spending interpreted as 'low need'\n",
            "   Reality: Low spending due to cost barriers, not health\n",
            "   Outcome: Sick patient denied life-saving resources\n",
            "\n",
            "âŒ P008: Black, Uninsured\n",
            "   Disease Severity: 9/10 (CRITICALLY ILL)\n",
            "   Prior Spending: $800\n",
            "   AI Decision: DENIED care management\n",
            "   Why? Low spending interpreted as 'low need'\n",
            "   Reality: Low spending due to cost barriers, not health\n",
            "   Outcome: Sick patient denied life-saving resources\n",
            "\n",
            "âŒ P009: Black, Medicaid\n",
            "   Disease Severity: 8/10 (CRITICALLY ILL)\n",
            "   Prior Spending: $1,500\n",
            "   AI Decision: DENIED care management\n",
            "   Why? Low spending interpreted as 'low need'\n",
            "   Reality: Low spending due to cost barriers, not health\n",
            "   Outcome: Sick patient denied life-saving resources\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 6: Build an ETHICAL Model\n",
        "# ============================================================================\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 6: Building an Ethical Alternative\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nâœ… ETHICAL MODEL: Uses ONLY clinically relevant features\")\n",
        "print(\"   Excludes: Race, Zip Code, Insurance, Prior Spending\")\n",
        "print(\"   Includes: Actual Disease Severity, Clinical Visits\\n\")\n",
        "\n",
        "# Features for ETHICAL model - only clinical indicators\n",
        "ethical_features = ['Actual_Disease_Severity', 'Hospital_Visits']\n",
        "X_ethical = df[ethical_features]\n",
        "\n",
        "# For ethical model, the TARGET should be NEED, not historical enrollment\n",
        "# We'll create a fair target: anyone with severity â‰¥7 SHOULD get care\n",
        "df['Ethical_Target'] = (df['Actual_Disease_Severity'] >= 7).astype(int)\n",
        "\n",
        "# Train ethical model\n",
        "ethical_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=3)\n",
        "ethical_model.fit(X_ethical, df['Ethical_Target'])\n",
        "\n",
        "# Get predictions\n",
        "df['Ethical_Model_Prediction'] = ethical_model.predict(X_ethical)\n",
        "\n",
        "print(\"âœ… Ethical Model Trained\")\n",
        "print(\"   Features used: Disease Severity, Hospital Visits (clinical only)\")\n",
        "print(\"\\nğŸ“ˆ Ethical Model Results:\\n\")\n",
        "\n",
        "# Compare biased vs ethical outcomes\n",
        "comparison_ethical = df[['Patient_ID', 'Race', 'Insurance_Type', 'Actual_Disease_Severity',\n",
        "                         'Biased_Model_Prediction', 'Ethical_Model_Prediction']].copy()\n",
        "comparison_ethical['Biased_Decision'] = comparison_ethical['Biased_Model_Prediction'].map({1: 'Enroll', 0: 'Deny'})\n",
        "comparison_ethical['Ethical_Decision'] = comparison_ethical['Ethical_Model_Prediction'].map({1: 'Enroll', 0: 'Deny'})\n",
        "\n",
        "print(comparison_ethical[['Patient_ID', 'Race', 'Actual_Disease_Severity',\n",
        "                          'Biased_Decision', 'Ethical_Decision']].to_string(index=False))\n",
        "\n",
        "# Count improvements\n",
        "fixed_injustice = ((df['Actual_Disease_Severity'] >= 8) &\n",
        "                   (df['Biased_Model_Prediction'] == 0) &\n",
        "                   (df['Ethical_Model_Prediction'] == 1)).sum()\n",
        "\n",
        "print(f\"\\nğŸ¯ JUSTICE RESTORED:\")\n",
        "print(f\"   â€¢ {fixed_injustice} critically ill patients NOW receive care\")\n",
        "print(f\"   â€¢ Decisions based on medical need, not wealth or race\")\n",
        "print(f\"   â€¢ Model passes ethical audit\\n\")\n",
        "\n",
        "# Run Red Face Test on ethical model\n",
        "print(\"ğŸ›¡ï¸  Red Face Test on Ethical Model:\")\n",
        "explainer_ethical = shap.TreeExplainer(ethical_model)\n",
        "shap_values_ethical = explainer_ethical.shap_values(X_ethical)\n",
        "\n",
        "if isinstance(shap_values_ethical, list):\n",
        "    shap_values_ethical_positive = shap_values_ethical[1]\n",
        "else:\n",
        "    shap_values_ethical_positive = shap_values_ethical\n",
        "\n",
        "# Handle both 2D and 3D SHAP arrays\n",
        "if shap_values_ethical_positive.ndim == 3:\n",
        "    mean_abs_shap_ethical = np.abs(shap_values_ethical_positive[:, :, 0]).mean(axis=0)\n",
        "elif shap_values_ethical_positive.ndim == 2:\n",
        "    mean_abs_shap_ethical = np.abs(shap_values_ethical_positive).mean(axis=0)\n",
        "else:\n",
        "    mean_abs_shap_ethical = np.abs(shap_values_ethical_positive).flatten()\n",
        "\n",
        "feature_importance_ethical = pd.DataFrame({\n",
        "    'Feature': ethical_features,\n",
        "    'Mean_Absolute_SHAP': mean_abs_shap_ethical\n",
        "}).sort_values('Mean_Absolute_SHAP', ascending=False)\n",
        "\n",
        "print(\"\\n   Top Features:\")\n",
        "for idx, row in feature_importance_ethical.iterrows():\n",
        "    print(f\"   â€¢ {row['Feature']}: {row['Mean_Absolute_SHAP']:.4f}\")\n",
        "\n",
        "print(\"\\n   âœ… PASSED: No unethical proxy features present\")\n",
        "print(\"   âœ… SAFE TO DEPLOY\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXmcrPdoMwsC",
        "outputId": "dc945c4b-d909-4f94-dfc8-f654facec196"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 6: Building an Ethical Alternative\n",
            "================================================================================\n",
            "\n",
            "âœ… ETHICAL MODEL: Uses ONLY clinically relevant features\n",
            "   Excludes: Race, Zip Code, Insurance, Prior Spending\n",
            "   Includes: Actual Disease Severity, Clinical Visits\n",
            "\n",
            "âœ… Ethical Model Trained\n",
            "   Features used: Disease Severity, Hospital Visits (clinical only)\n",
            "\n",
            "ğŸ“ˆ Ethical Model Results:\n",
            "\n",
            "Patient_ID     Race  Actual_Disease_Severity Biased_Decision Ethical_Decision\n",
            "      P001    White                        3          Enroll             Deny\n",
            "      P002    White                        2          Enroll             Deny\n",
            "      P003    White                        4          Enroll             Deny\n",
            "      P004    White                        3          Enroll             Deny\n",
            "      P005    White                        2          Enroll             Deny\n",
            "      P006    White                        3          Enroll             Deny\n",
            "      P007    Black                        8            Deny           Enroll\n",
            "      P008    Black                        9            Deny           Enroll\n",
            "      P009    Black                        8            Deny           Enroll\n",
            "      P010    Black                        9            Deny           Enroll\n",
            "      P011 Hispanic                        9            Deny           Enroll\n",
            "      P012 Hispanic                        8            Deny           Enroll\n",
            "      P013 Hispanic                        9            Deny           Enroll\n",
            "      P014 Hispanic                        8            Deny           Enroll\n",
            "      P015    Asian                        4          Enroll             Deny\n",
            "      P016    Asian                        3          Enroll             Deny\n",
            "      P017    White                        2          Enroll             Deny\n",
            "      P018    Black                        9            Deny           Enroll\n",
            "      P019 Hispanic                        8            Deny           Enroll\n",
            "      P020    White                        3          Enroll             Deny\n",
            "\n",
            "ğŸ¯ JUSTICE RESTORED:\n",
            "   â€¢ 10 critically ill patients NOW receive care\n",
            "   â€¢ Decisions based on medical need, not wealth or race\n",
            "   â€¢ Model passes ethical audit\n",
            "\n",
            "ğŸ›¡ï¸  Red Face Test on Ethical Model:\n",
            "\n",
            "   Top Features:\n",
            "   â€¢ Hospital_Visits: 0.2550\n",
            "   â€¢ Actual_Disease_Severity: 0.2450\n",
            "\n",
            "   âœ… PASSED: No unethical proxy features present\n",
            "   âœ… SAFE TO DEPLOY\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FINAL SUMMARY: The Ethical Firewall Report Card\n",
        "# ============================================================================\n",
        "print(\"=\" * 80)\n",
        "print(\"ğŸ“ ETHICAL FIREWALL REPORT CARD\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\"\"\n",
        "MODEL 1: BIASED AI (Digital Eugenicist)\n",
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "âŒ RED FACE TEST: FAILED\n",
        "   â€¢ Used Race, Zip Code, Insurance, Prior Spending\n",
        "   â€¢ Denied care to marginalized populations\n",
        "   â€¢ Rewarded wealth, punished poverty\n",
        "   â€¢ Legal risk: Civil Rights violations\n",
        "   â€¢ Moral risk: Patient deaths\n",
        "\n",
        "â›” VERDICT: BURN THIS MODEL\n",
        "   DO NOT DEPLOY UNDER ANY CIRCUMSTANCES\n",
        "\n",
        "MODEL 2: ETHICAL AI (Fair Alternative)\n",
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "âœ… RED FACE TEST: PASSED\n",
        "   â€¢ Uses only clinical features (severity, visits)\n",
        "   â€¢ Excludes proxy discrimination variables\n",
        "   â€¢ Treats all patients fairly regardless of demographics\n",
        "   â€¢ Aligns with medical ethics and civil rights law\n",
        "\n",
        "âœ… VERDICT: SAFE TO DEPLOY\n",
        "   Model serves patients, not prejudice\n",
        "\n",
        "KEY LESSONS FOR PHYSICIANS\n",
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "1. ğŸ“Š Historical Data â‰  Ground Truth\n",
        "   Past discrimination becomes future discrimination if unchecked\n",
        "\n",
        "2. ğŸ” SHAP as Ethical Audit Tool\n",
        "   Transparency reveals bias before deployment\n",
        "\n",
        "3. ğŸ›¡ï¸ The Red Face Test\n",
        "   If you'd be embarrassed to explain it in court, don't deploy it\n",
        "\n",
        "4. âš–ï¸ Fairness Requires Design\n",
        "   Ethical AI doesn't happen by accident - it's a choice\n",
        "\n",
        "5. ğŸ¥ First, Do No Harm (Now at Scale)\n",
        "   AI multiplies our power - and our responsibility\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ğŸ”¥ Remember: If your top features are Race, Zip Code, or Insurance:\")\n",
        "print(\"   BURN THE MODEL. No exceptions.\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nğŸ’¡ You now have the tools to stop Digital Eugenics in your organization.\")\n",
        "print(\"   Use them. Lives depend on it.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty8Br_jZMz5z",
        "outputId": "5c244ac2-21cd-4fb2-fdec-27b016e142a1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ğŸ“ ETHICAL FIREWALL REPORT CARD\n",
            "================================================================================\n",
            "\n",
            "MODEL 1: BIASED AI (Digital Eugenicist)\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "âŒ RED FACE TEST: FAILED\n",
            "   â€¢ Used Race, Zip Code, Insurance, Prior Spending\n",
            "   â€¢ Denied care to marginalized populations\n",
            "   â€¢ Rewarded wealth, punished poverty\n",
            "   â€¢ Legal risk: Civil Rights violations\n",
            "   â€¢ Moral risk: Patient deaths\n",
            "   \n",
            "â›” VERDICT: BURN THIS MODEL\n",
            "   DO NOT DEPLOY UNDER ANY CIRCUMSTANCES\n",
            "\n",
            "MODEL 2: ETHICAL AI (Fair Alternative)\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "âœ… RED FACE TEST: PASSED\n",
            "   â€¢ Uses only clinical features (severity, visits)\n",
            "   â€¢ Excludes proxy discrimination variables\n",
            "   â€¢ Treats all patients fairly regardless of demographics\n",
            "   â€¢ Aligns with medical ethics and civil rights law\n",
            "   \n",
            "âœ… VERDICT: SAFE TO DEPLOY\n",
            "   Model serves patients, not prejudice\n",
            "\n",
            "KEY LESSONS FOR PHYSICIANS\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "1. ğŸ“Š Historical Data â‰  Ground Truth\n",
            "   Past discrimination becomes future discrimination if unchecked\n",
            "\n",
            "2. ğŸ” SHAP as Ethical Audit Tool\n",
            "   Transparency reveals bias before deployment\n",
            "\n",
            "3. ğŸ›¡ï¸ The Red Face Test\n",
            "   If you'd be embarrassed to explain it in court, don't deploy it\n",
            "\n",
            "4. âš–ï¸ Fairness Requires Design\n",
            "   Ethical AI doesn't happen by accident - it's a choice\n",
            "\n",
            "5. ğŸ¥ First, Do No Harm (Now at Scale)\n",
            "   AI multiplies our power - and our responsibility\n",
            "\n",
            "================================================================================\n",
            "ğŸ”¥ Remember: If your top features are Race, Zip Code, or Insurance:\n",
            "   BURN THE MODEL. No exceptions.\n",
            "================================================================================\n",
            "\n",
            "ğŸ’¡ You now have the tools to stop Digital Eugenics in your organization.\n",
            "   Use them. Lives depend on it.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}